{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab 2: To work on Azure RDD, Dataframes and SQL"
      ],
      "metadata": {
        "id": "J-SCt1ZvWPt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Objective\n",
        "\n",
        "* To Create Spark RDD\n",
        "* To create Spark DataFrame\n",
        "* To create Spark SQL"
      ],
      "metadata": {
        "id": "YQj9E7NxWlF3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anfT08GBX4CT"
      },
      "source": [
        "## Spark RDD (Resilient Distributed Dataset) \n",
        "\n",
        "* RDD is an immutable fault-tolerant, distributed collection of objects that can be operated on in parallel. \n",
        "\n",
        "* An RDD can contain any type of object and is created by loading an external dataset or distributing a collection from the driver program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rBjhjiBX8Wq"
      },
      "source": [
        "### Creating RDD in Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dspsRyUlX98-"
      },
      "source": [
        "#####  There are three ways to create an RDD in Spark.\n",
        "\n",
        "* Parallelizing already existing collection in driver program.\n",
        "* Referencing a dataset in an external storage system (e.g. HDFS, Hbase, shared file system).\n",
        "* Creating RDD from already existing RDDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDgcDZT07RJH"
      },
      "outputs": [],
      "source": [
        "MyRdd1 = sc.parallelize([(\"maths\",92),(\"english\",75),(\"SCiences\",85),(\"Social\",90)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muoxlAWO8AAe"
      },
      "outputs": [],
      "source": [
        "MyRdd1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9khHrI3w8C7-"
      },
      "outputs": [],
      "source": [
        "MyRdd1.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6MHkIi0YF5g"
      },
      "source": [
        "### Creating an rdd by reading a file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AyKEpti852j"
      },
      "outputs": [],
      "source": [
        "MyRdd2 = sc.textFile('/temp_data.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukEz82E3-Y_L"
      },
      "outputs": [],
      "source": [
        "MyRdd2.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iid8zfbd-c4Y"
      },
      "outputs": [],
      "source": [
        "MyRdd3 = MyRdd2.map(lambda s: s.split('\\t'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9vwiPFRAmyf"
      },
      "outputs": [],
      "source": [
        "type(MyRdd3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9A2eJ9UAoBL"
      },
      "outputs": [],
      "source": [
        "MyRdd3.take(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4mr-RalYPPS"
      },
      "source": [
        "### RDDs support two types of operations:\n",
        "* Transformations are operations (such as map, filter, join, union, and so on) that are performed on an RDD and which yield a new RDD containing the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YV8FYGcYS3s"
      },
      "source": [
        "* Transformations in Spark are “lazy”, meaning that they do not compute their results right away. \n",
        "* They just “remember” the operation to be performed and the dataset (e.g., file) to which the operation is to be    performed. \n",
        "* The transformations are only actually computed when an action is called and the result is returned to the driver program. \n",
        "* This design enables Spark to run more efficiently. For example, if a big file was transformed in various ways and passed to first action, Spark would only process and return the result for the first line, rather than do the work for the entire file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ea7w30cAxwo"
      },
      "outputs": [],
      "source": [
        "intRdd = sc.parallelize([10,20,30,40,50])\n",
        "mapRdd = intRdd.map(lambda x : x**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S2Y1-nRBokq"
      },
      "outputs": [],
      "source": [
        "mapRdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b9gskO27yCT"
      },
      "source": [
        "##  Spark DataFrame \n",
        "\n",
        "#### A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. \n",
        "<br> The list that defines the columns and the types within those columns is called the schema. \n",
        "<br> One can think of a DataFrame as a spreadsheet with named columns.\n",
        "<br> A spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.\n",
        "<br> The reason for putting the data on more than one computer should be intuitive: \n",
        "<br>     either the data is too large to fit on one machine or \n",
        "<br>     it would simply take too long to perform that computation on one machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcrtLQG57yCU"
      },
      "source": [
        "#### Create a dataframe with one column containing 100 rows with values from 0 to 99.\n",
        "This range of numbers represents a distributed collection. \n",
        "<br> When run on a cluster, each part of this range of numbers exists on a different executor. \n",
        "<br> This is a Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ird4hwpA7yCV"
      },
      "outputs": [],
      "source": [
        "myRange = spark.range(100).toDF('number')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q_buWTy7yCY"
      },
      "outputs": [],
      "source": [
        "myRange.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76yGrAOB7yCb"
      },
      "outputs": [],
      "source": [
        "myRange.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUvcIaOZ7yCf"
      },
      "outputs": [],
      "source": [
        "type(myRange)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4LhUiU47yCi"
      },
      "outputs": [],
      "source": [
        "myDF = spark.createDataFrame([[1, 'Alice', 30],\n",
        "                              [2, 'Bob', 28],\n",
        "                              [3, 'Cathy', 31], \n",
        "                              [4, 'Dave', 56]], ['Id', 'Name', 'Age'])\n",
        "\n",
        "myDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvuvjLyx7yCm"
      },
      "outputs": [],
      "source": [
        "myDF.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmBFLQsO7yCp"
      },
      "source": [
        "## DataFrame Transformations & Actions\n",
        "\n",
        "### Transformations\n",
        "In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created.\n",
        "<br> To “change” a DataFrame, you need to instruct Spark how you would like to modify it to do what you want.\n",
        "<br> These instructions are called transformations.\n",
        "<br> Transformations are the core of how you express your business logic using Spark.\n",
        "<br> Transformations are simply ways of specifying different series of data manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuhUUFF27yCq"
      },
      "outputs": [],
      "source": [
        "myRange.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HridY7-D7yCt"
      },
      "outputs": [],
      "source": [
        "divisBy2 = myRange.where(\"id % 2 = 0\")\n",
        "divisBy2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IazheZsN7yCx"
      },
      "source": [
        "Notice that these return no output. <br>This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p44qdTlT7yCy"
      },
      "source": [
        "### Actions\n",
        "Transformations allow us to build up our logical transformation plan. \n",
        "<br> To trigger the computation, we run an action.\n",
        "<br> An action instructs Spark to compute a result from a series of transformations. \n",
        "<br> The simplest action is count, which gives us the total number of records in the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMEQR_on7yCz",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "divisBy2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdZ9U4zt7yC3"
      },
      "outputs": [],
      "source": [
        "divisBy2.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfMvgxgcVnQd"
      },
      "outputs": [],
      "source": [
        "testRDD = sc.textFile(\"/test.csv\")\n",
        "print(\"Total Records with header: \", testRDD.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv0i6dme7yFR"
      },
      "source": [
        "### Spark SQL\n",
        "With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. \n",
        "<br>There is no performance difference between writing SQL queries or writing DataFrame code, <br>they both “compile” to the same underlying plan that we specify in DataFrame code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8S6k65O7yFS"
      },
      "outputs": [],
      "source": [
        "## Create view/table\n",
        "trainDF.createOrReplaceTempView(\"trainDFTable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nto7KffE7yFU"
      },
      "outputs": [],
      "source": [
        "## Verify Dataframe\n",
        "trainDF.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ1jF9327yFW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## Verify Table\n",
        "spark.sql(\"SELECT * FROM trainDFTable LIMIT 2\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyYVO7xrVnQd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}